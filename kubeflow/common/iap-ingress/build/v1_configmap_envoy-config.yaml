apiVersion: v1
data:
  healthcheck_route.yaml: |
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: default-routes
      namespace: $(namespace)
    spec:
      hosts:
      - "*"
      gateways:
      - kubeflow-gateway
      http:
      - match:
        - uri:
            exact: /healthz
        route:
        - destination:
            port:
              number: 80
            host: whoami-app.kubeflow.svc.cluster.local
      - match:
        - uri:
            exact: /whoami
        route:
        - destination:
            port:
              number: 80
            host: whoami-app.kubeflow.svc.cluster.local
    ---
    apiVersion: networking.istio.io/v1alpha3
    kind: Gateway
    metadata:
      name: kubeflow-gateway
      namespace: $(namespace)
    spec:
      selector:
        istio: ingressgateway
      servers:
      - port:
          number: 80
          name: http
          protocol: HTTP
        hosts:
        - "*"
  policy.yaml: |
    apiVersion: security.istio.io/v1beta1
    kind: RequestAuthentication
    metadata:
      name: ingress-jwt
    spec:
      selector:
        matchLabels:
          app: istio-ingressgateway
      jwtRules:
      - audiences:
        - JWT_AUDIENCE
        issuer: https://cloud.google.com/iap
        jwksUri: https://www.gstatic.com/iap/verify/public_key-jwk
        fromHeaders:
        - name: x-goog-iap-jwt-assertion
  setup_backend.sh: "#!/usr/bin/env bash\n#\n# A simple shell script to configure
    the JWT audience used with ISTIO\n\nset -x\n[ -z ${NAMESPACE} ] && echo Error
    NAMESPACE must be set && exit 1\n[ -z ${SERVICE} ] && echo Error SERVICE must
    be set && exit 1\n[ -z ${INGRESS_NAME} ] && echo Error INGRESS_NAME must be set
    && exit 1\n\n__dir=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\nPROJECT=$(curl
    -s -H \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/project/project-id)\nif
    [ -z ${PROJECT} ]; then\n    echo Error unable to fetch PROJECT from compute metadata\n
    \   exit 1\nfi\n\nPROJECT_NUM=$(curl -s -H \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id)\nif
    [ -z ${PROJECT_NUM} ]; then\n    echo Error unable to fetch PROJECT_NUM from compute
    metadata\n    exit 1\nfi\n\n# Activate the service account\nif [ ! -z \"${GOOGLE_APPLICATION_CREDENTIALS}\"
    ]; then\n    # As of 0.7.0 we should be using workload identity and never setting
    GOOGLE_APPLICATION_CREDENTIALS.\n    # But we kept this for backwards compatibility
    but can remove later.\n    gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\nfi\n\n#
    Print out the config for debugging\ngcloud config list\ngcloud auth list\n\nset_jwt_policy
    () {\n    NODE_PORT=$(kubectl --namespace=${NAMESPACE} get svc ${SERVICE} -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n
    \   echo \"node port is ${NODE_PORT}\"\n\n    BACKEND_NAME=\"\"\n    while [[
    -z ${BACKEND_NAME} ]]; do\n    BACKENDS=$(kubectl --namespace=${NAMESPACE} get
    ingress ${INGRESS_NAME} -o jsonpath='{.metadata.annotations.ingress\\.kubernetes\\.io/backends}')\n
    \   echo \"fetching backends info with ${INGRESS_NAME}: ${BACKENDS}\"\n    BACKEND_NAME=$(echo
    $BACKENDS | grep -o \"k8s-be-${NODE_PORT}--[0-9a-z]\\+\")\n    echo \"backend
    name is ${BACKEND_NAME}\"\n    sleep 2\n    done\n\n    BACKEND_ID=\"\"\n    while
    [[ -z ${BACKEND_ID} ]]; do\n    BACKEND_ID=$(gcloud compute --project=${PROJECT}
    backend-services list --filter=name~${BACKEND_NAME} --format='value(id)')\n    echo
    \"Waiting for backend id PROJECT=${PROJECT} NAMESPACE=${NAMESPACE} SERVICE=${SERVICE}
    filter=name~${BACKEND_NAME}\"\n    sleep 2\n    done\n    echo BACKEND_ID=${BACKEND_ID}\n\n
    \   JWT_AUDIENCE=\"/projects/${PROJECT_NUM}/global/backendServices/${BACKEND_ID}\"\n
    \   echo \"Upsert RequestAuthentication with JWT audience: ${JWT_AUDIENCE}\"\n
    \   sed \"s|JWT_AUDIENCE|${JWT_AUDIENCE}|\" ${__dir}/policy.yaml | kubectl apply
    -n ${NAMESPACE} -f -\n\n    echo \"Clearing lock on service annotation\"\n    kubectl
    patch svc \"${SERVICE}\" -n istio-system -p \"{\\\"metadata\\\": { \\\"annotations\\\":
    {\\\"backendlock\\\": \\\"\\\" }}}\"\n}\n\nwhile true; do\n    set_jwt_policy\n
    \   # Every 5 minutes recheck the JWT policy and reset it if the backend has changed
    for some reason.\n    # This follows Kubernetes level based design.\n    # We
    have at least one report see \n    # https://github.com/kubeflow/kubeflow/issues/4342#issuecomment-544653657\n
    \   # of the backend id changing over time.\n    sleep 300\ndone\n"
  setup_cloudendpoints.sh: "#!/bin/bash\n#\n# A simple shell script to configure a
    cloud endpoint\nset -x\n[ -z ${NAMESPACE} ] && echo Error NAMESPACE must be set
    && exit 1\n[ -z ${SERVICE} ] && echo Error SERVICE must be set && exit 1\n[ -z
    ${INGRESS_NAME} ] && echo Error INGRESS_NAME must be set && exit 1\n[ -z ${ENDPOINT_NAME}
    ] && echo Error ENDPOINT_NAME must be set && exit 1\n\n__dir=\"$(cd \"$(dirname
    \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\nPROJECT=$(curl -s -H \"Metadata-Flavor:
    Google\" http://metadata.google.internal/computeMetadata/v1/project/project-id)\nif
    [ -z ${PROJECT} ]; then\n    echo Error unable to fetch PROJECT from compute metadata\n
    \   exit 1\nfi\n\nPROJECT_NUM=$(curl -s -H \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id)\nif
    [ -z ${PROJECT_NUM} ]; then\n    echo Error unable to fetch PROJECT_NUM from compute
    metadata\n    exit 1\nfi\n\n# Activate the service account\nif [ ! -z \"${GOOGLE_APPLICATION_CREDENTIALS}\"
    ]; then\n    # As of 0.7.0 we should be using workload identity and never setting
    GOOGLE_APPLICATION_CREDENTIALS.\n    # But we kept this for backwards compatibility
    but can remove later.\n    gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\nfi\n\n#
    Print out the config for debugging\ngcloud config list\ngcloud auth list\n\nset_endpoint
    () {\n    NODE_PORT=$(kubectl --namespace=${NAMESPACE} get svc ${SERVICE} -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n
    \   echo \"[DEBUG] node port is ${NODE_PORT}\"\n\n    BACKEND_NAME=\"\"\n    while
    [[ -z ${BACKEND_NAME} ]]; do\n    BACKENDS=$(kubectl --namespace=${NAMESPACE}
    get ingress ${INGRESS_NAME} -o jsonpath='{.metadata.annotations.ingress\\.kubernetes\\.io/backends}')\n
    \   echo \"[DEBUG] fetching backends info with ${INGRESS_NAME}: ${BACKENDS}\"\n
    \   BACKEND_NAME=$(echo $BACKENDS | grep -o \"k8s-be-${NODE_PORT}--[0-9a-z]\\+\")\n
    \   echo \"[DEBUG] backend name is ${BACKEND_NAME}\"\n    sleep 2\n    done\n\n
    \   BACKEND_ID=\"\"\n    while [[ -z ${BACKEND_ID} ]]; do\n    BACKEND_ID=$(gcloud
    compute --project=${PROJECT} backend-services list --filter=name~${BACKEND_NAME}
    --format='value(id)')\n    echo \"[DEBUG] Waiting for backend id PROJECT=${PROJECT}
    NAMESPACE=${NAMESPACE} SERVICE=${SERVICE} filter=name~${BACKEND_NAME}\"\n    sleep
    2\n    done\n    echo BACKEND_ID=${BACKEND_ID}\n\n    JWT_AUDIENCE=\"/projects/${PROJECT_NUM}/global/backendServices/${BACKEND_ID}\"\n
    \   \n    # We use a regular expression to obtain the IP address of the target
    Ingress, assuming IPv4 standard.\n    INGRESS_TARGET_IP=$(kubectl get ingress
    --all-namespaces | grep -E -o \"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\")\n
    \   \n    echo \"[DEBUG] ENDPOINT_NAME = ${ENDPOINT_NAME}\"\n    echo \"[DEBUG]
    INGRESS_TARGET_IP = ${INGRESS_TARGET_IP}\"\n    echo \"[DEBUG] JWT_AUDIENCE =
    ${JWT_AUDIENCE}\"\n    \n    # Create OpenAPI specification for the RESTful Cloud
    Endpoint\n    sed \"s|JWT_AUDIENCE|${JWT_AUDIENCE}|;s|ENDPOINT_NAME|${ENDPOINT_NAME}|;s|INGRESS_TARGET_IP|${INGRESS_TARGET_IP}|\"
    /var/envoy-config/swagger_template.yaml > openapi.yaml\n    \n    # Deploy and
    enable the endpoint\n    gcloud endpoints services deploy openapi.yaml\n    gcloud
    services enable ${ENDPOINT_NAME}\n\n    # Create IAM resources used by the endpoint\n
    \   gcloud endpoints services add-iam-policy-binding ${ENDPOINT_NAME} \\\n        --member
    serviceAccount:${SERVICE_ACCOUNTNAME} \\\n        --role roles/servicemanagement.serviceController\n
    \   gcloud projects add-iam-policy-binding ${PROJECT} \\\n        --member serviceAccount:${SERVICE_ACCOUNTNAME}
    \\\n        --role roles/cloudtrace.agent\n}\n\nwhile true; do\n    set_endpoint\n
    \   echo \"Sleeping 30 seconds...\"\n    sleep 30\ndone\n"
  swagger_template.yaml: |
    swagger: "2.0"
    info:
      description: "wildcard config for any HTTP service."
      title: "General HTTP Service."
      version: "1.0.0"
    host: "ENDPOINT_NAME"
    x-google-endpoints:
    - name: "ENDPOINT_NAME"
      target: "INGRESS_TARGET_IP"
    basePath: "/"
    consumes:
    - "application/json"
    produces:
    - "application/json"
    schemes:
    - "http"
    - "https"
    paths:
      "/**":
        get:
          operationId: Get
          responses:
            '200':
              description: Get
            default:
              description: Error
        delete:
          operationId: Delete
          responses:
            '204':
              description: Delete
            default:
              description: Error
        patch:
          operationId: Patch
          responses:
            '200':
              description: Patch
            default:
              description: Error
        post:
          operationId: Post
          responses:
            '200':
              description: Post
            default:
              description: Error
        put:
          operationId: Put
          responses:
            '200':
              description: Put
            default:
              description: Error
    security:
    - google_jwt: []
    securityDefinitions:
      google_jwt:
        authorizationUrl: ""
        flow: "implicit"
        type: "oauth2"
        x-google-issuer: "https://cloud.google.com/iap"
        x-google-jwks_uri: "https://www.gstatic.com/iap/verify/public_key-jwk"
        x-google-audiences: "JWT_AUDIENCE"
  update_backend.sh: "#!/bin/bash\n#\n# A simple shell script to configure the health
    checks by using gcloud.\nset -x\n\n[ -z ${NAMESPACE} ] && echo Error NAMESPACE
    must be set && exit 1\n[ -z ${SERVICE} ] && echo Error SERVICE must be set &&
    exit 1\n[ -z ${INGRESS_NAME} ] && echo Error INGRESS_NAME must be set && exit
    1\n\nPROJECT=$(curl -s -H \"Metadata-Flavor: Google\" http://metadata.google.internal/computeMetadata/v1/project/project-id)\nif
    [ -z ${PROJECT} ]; then\n    echo Error unable to fetch PROJECT from compute metadata\n
    \   exit 1\nfi\n\nif [[ ! -z \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]]; then\n
    \   # TODO(jlewi): As of 0.7 we should always be using workload identity. We can
    remove it post 0.7.0 once we have workload identity\n    # fully working\n    #
    Activate the service account, allow 5 retries\n    for i in {1..5}; do gcloud
    auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS} &&
    break || sleep 10; done\nfi\n\nset_health_check () {\n    NODE_PORT=$(kubectl
    --namespace=${NAMESPACE} get svc ${SERVICE} -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n
    \   echo node port is ${NODE_PORT}\n\n    while [[ -z ${BACKEND_NAME} ]]; do\n
    \   BACKENDS=$(kubectl --namespace=${NAMESPACE} get ingress ${INGRESS_NAME} -o
    jsonpath='{.metadata.annotations.ingress\\.kubernetes\\.io/backends}')\n    echo
    \"fetching backends info with ${INGRESS_NAME}: ${BACKENDS}\"\n    BACKEND_NAME=$(echo
    $BACKENDS | grep -o \"k8s-be-${NODE_PORT}--[0-9a-z]\\+\")\n    echo \"backend
    name is ${BACKEND_NAME}\"\n    sleep 2\n    done\n\n    while [[ -z ${BACKEND_SERVICE}
    ]]; do\n    BACKEND_SERVICE=$(gcloud --project=${PROJECT} compute backend-services
    list --filter=name~${BACKEND_NAME} --uri);\n    echo \"Waiting for the backend-services
    resource PROJECT=${PROJECT} BACKEND_NAME=${BACKEND_NAME} SERVICE=${SERVICE}...\";\n
    \   sleep 2;\n    done\n\n    while [[ -z ${HEALTH_CHECK_URI} ]]; do\n    HEALTH_CHECK_URI=$(gcloud
    compute --project=${PROJECT} health-checks list --filter=name~${BACKEND_NAME}
    --uri);\n    echo \"Waiting for the healthcheck resource PROJECT=${PROJECT} NODEPORT=${NODE_PORT}
    SERVICE=${SERVICE}...\";\n    sleep 2;\n    done\n    echo health check URI is
    ${HEALTH_CHECK_URI}\n\n    # See: https://cloud.google.com/service-mesh/docs/iap-integration#deploying_the_load_balancer\n
    \   HC_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway
    -o jsonpath='{.spec.ports[?(@.name==\"status-port\")].nodePort}')\n    echo Setting
    BackendConfig healthCheck.port to: ${HC_INGRESS_PORT}\n    kubectl patch backendconfig
    iap-backendconfig -n ${NAMESPACE} --type json -p '[{\"op\": \"replace\", \"path\":
    \"/spec/healthCheck/port\", \"value\": '${HC_INGRESS_PORT}'}]'\n    \n    HC_INGRESS_PATH=$(kubectl
    get pods -n istio-system -l app=istio-ingressgateway -o jsonpath='{.items[0].spec.containers[?(@.name==\"istio-proxy\")].readinessProbe.httpGet.path}')\n
    \   echo Setting BackendConfig healthCheck.requestPath to ${HC_INGRESS_PATH}\n
    \   kubectl patch backendconfig iap-backendconfig -n ${NAMESPACE} --type json
    -p '[{\"op\": \"replace\", \"path\": \"/spec/healthCheck/requestPath\", \"value\":
    \"'${HC_INGRESS_PATH}'\"}]'\n}\n\nwhile true; do\n    set_health_check\n    echo
    \"Backend updated successfully. Waiting 1 hour before updating again.\"\n    sleep
    3600\ndone\n"
kind: ConfigMap
metadata:
  labels:
    kustomize.component: iap-ingress
  name: envoy-config
  namespace: istio-system
